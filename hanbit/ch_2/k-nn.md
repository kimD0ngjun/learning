# K 최근접 이웃 알고리즘
[위키피디아 링크 설명](https://ko.wikipedia.org/wiki/K-%EC%B5%9C%EA%B7%BC%EC%A0%91_%EC%9D%B4%EC%9B%83_%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98)
```python
from sklearn.neighbors import KNeighborsClassifier

kn = KNeighborsClassifier()
kn.fit(fish_data, fish_target)
kn.score(fish_data, fish_target)
```
- K 최근접 이웃 알고리즘은 `사이킷런.이웃들` 모듈에 `KNeighborsClassifier`로 구현

### 1. 개요
- **데이터 간의 거리 개념**과 관련
- 여기서 말하는 거리는 유클리드 거리, 맨하탄 거리, 코사인 유사도 등등 다양한 개념 포괄
- 새로운 데이터가 들어오면, **기존의 다른 데이터들과의 거리**를 측정
- 그 중, **가장 가까운 데이터 K개를 추출해서 다수 분류에 속하는 데이터에 포함된다고 결론**
- K는 보통 교차검증으로 결정

### 2. 특징
**장점**
- 단순함, 이해하기 쉬움
- 비선형 문제에서도 잘 동작하고 파라미터가 거의 없음

**단점**
- 딱 봐도 전체 탐색임. 속도 드럽게 느림
- 차원이 높아지면 거리 계산이 복잡해져서 비효율적
- 특성 스케일링 필요(특성의 편중 현상으로 데이터 자체가 다른 분류범위로 넘어갔다고 착각할 가능성이 높음)

### 3. 결론
- 사실 학습(훈련)하는 내용이 아무것도 없음, 오로지 예측 단계에서만 전부 연산
- **다른 알고리즘들은 학습 단계에서 미리 규칙을 만드는데, 얘는 그런 게 없음. '게으른 학습(lazy learning)'**
- 그래서 정규화나 표준화 등으로 특성 값의 범위를 통일하는 등의 특성 스케일링이 필요함